{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RashmiJK/PGP-AIML-MedicalAssistant-NLP/blob/main/medical_assistant_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CNz35ia6Bz3"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkRbhMJH6Bz3"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PBm5xaj6Bz3"
      },
      "source": [
        "The healthcare industry is rapidly evolving, with professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
        "\n",
        "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
        "\n",
        "To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xDPsqvO6Bz5"
      },
      "source": [
        "**Common Questions to Answer**\n",
        "\n",
        "**1. Diagnostic Assistance**: \"What are the common symptoms and treatments for pulmonary embolism?\"\n",
        "\n",
        "**2. Drug Information**: \"Can you provide the trade names of medications used for treating hypertension?\"\n",
        "\n",
        "**3. Treatment Plans**: \"What are the first-line options and alternatives for managing rheumatoid arthritis?\"\n",
        "\n",
        "**4. Specialty Knowledge**: \"What are the diagnostic steps for suspected endocrine disorders?\"\n",
        "\n",
        "**5. Critical Care Protocols**: \"What is the protocol for managing sepsis in a critical care unit?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARPKFwm6Bz4"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOElOEXq6Bz4"
      },
      "source": [
        "As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges. The objective is to **understand** issues like information overload, **apply** AI techniques to streamline decision-making, **analyze** its impact on diagnostics and patient outcomes, **evaluate** its potential to standardize care practices, and **create** a functional prototype demonstrating its feasibility and effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9EvAnkSpZf"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw5LievCSru2"
      },
      "source": [
        "The **Merck Manuals** are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.\n",
        "\n",
        "The manual is provided as a PDF with over 4,000 pages divided into 23 sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnwETBOE6Bz5"
      },
      "source": [
        "## 1 - Installing and Importing Necessary Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e27b85f"
      },
      "source": [
        "**Set Google Colab to use the T4 GPU**\n",
        "\n",
        "Install `llama-cpp-python` with GPU acceleration. The wheel build is essential; ignore other errors. Then restart runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `llama-cpp-python` is a Python wrapper for llama.cpp, a universal LLM inference library that runs models efficiently using the GGUF file format.\n",
        "\n",
        "- GGUF (GGML Universal File) is a binary format storing model weights and metadata in a single file. It uses quantization to reduce precision, decreasing memory usage and increasing inference speed.\n",
        "\n",
        "- Model Compatibility: Supports any GGUF-converted model including Llama, Mistral, CodeLlama, Gemma, and Qwen.\n",
        "\n",
        "- `Llama()` class: Main interface for loading and running models\n",
        "\n",
        "- `hf_hub_download()`: A function from the Hugging Face Hub library to download specific files from Hugging Face repositories with automatic caching"
      ],
      "metadata": {
        "id": "z517xAQCqNgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4GgLhZhUM4V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1116b57-9c77-4daa-9f3f-0cb86aa0d083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m349.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m316.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m329.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m229.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installation for GPU llama-cpp-python: Downloads and compiles the library with GPU acceleration enabled.\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0VOckDVkWGei"
      },
      "outputs": [],
      "source": [
        "# Install the libraries & downloading models from HF Hub\n",
        "!pip install huggingface_hub pandas tiktoken==0.6.0 pymupdf==1.25.1 langchain==0.3.25 langchain-community==0.3.25 chromadb sentence-transformers numpy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RTY9GN4oWK3g"
      },
      "outputs": [],
      "source": [
        "# Libraries for downloading and loading the llm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtZWqj0wFTS1"
      },
      "source": [
        "## 2 - Question Answering using LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq1lhM4WFTS2"
      },
      "source": [
        "### 2.1 - Download and load the Mistral model\n",
        "| Model | Repository | File/Name | Model card |\n",
        "|-------|------------|-----------|---------|\n",
        "| Mistral-7B-Instruct-v0.2 | `TheBloke/Mistral-7B-Instruct-v0.2-GGUF` | `mistral-7b-instruct-v0.2.Q6_K.gguf` | https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONZL0PJNFTS2"
      },
      "outputs": [],
      "source": [
        "# Define the model repository and filename for the Mistral-7B-Instruct-v0.2 GGUF model.\n",
        "model_repo = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_file = \"mistral-7b-instruct-v0.2.Q6_K.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y-lNyJJFTS3"
      },
      "outputs": [],
      "source": [
        "# Download the model\n",
        "model_path = hf_hub_download(\n",
        "    repo_id= model_repo,\n",
        "    filename= model_file\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhVkc91BFTS3"
      },
      "outputs": [],
      "source": [
        "# Initialize the Llama model with the downloaded GGUF file.\n",
        "# model_path: path to the GGUF model file.\n",
        "# n_ctx: context window size (determines how much text the model can process at once).\n",
        "# n_gpu_layers: number of layers to offload to the GPU for acceleration.\n",
        "# n_batch: batch size for processing.\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2300,\n",
        "    n_gpu_layers=38,\n",
        "    n_batch=512\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzzkvIXvFTS4"
      },
      "source": [
        "### 2.2 - Utility function `generate_response`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve3UOjcUFTS4"
      },
      "outputs": [],
      "source": [
        "def generate_response(query, max_tokens=128, temperature=0, top_p=0.95, top_k=50, repeat_penalty=1.0):\n",
        "    \"\"\"\n",
        "    Generates a response from the language model.\n",
        "\n",
        "    Args:\n",
        "        query (str): The input prompt for the model.\n",
        "        max_tokens (int, optional): The maximum number of tokens to generate. Defaults to 128.\n",
        "        temperature (float, optional): Controls the randomness of the output. Defaults to 0.\n",
        "        top_p (float, optional): Nucleus sampling parameter. Defaults to 0.95.\n",
        "        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n",
        "        repeat_penalty (float, optional): Penalizes repeated tokens. Defaults to 1.0.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text response.\n",
        "    \"\"\"\n",
        "    model_output = llm(\n",
        "            prompt=query,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            repeat_penalty=repeat_penalty\n",
        "        )\n",
        "    print(model_output)\n",
        "    return model_output['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ9Wd9VOFTS4"
      },
      "outputs": [],
      "source": [
        "generate_response(\"What treatment options are available for managing hypertension?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8YgK91SFjVY"
      },
      "source": [
        "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Q_QZ4OFjVa"
      },
      "outputs": [],
      "source": [
        "user_input = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "response(user_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6yxICeVFjVc"
      },
      "source": [
        "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO1OTE9CFjVd"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to define the query #2\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oflaoOGiFjVd"
      },
      "source": [
        "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFm5Tq7RFjVe"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to define the query #3\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUUqY4FbFjVe"
      },
      "source": [
        "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGmG9hYzFjVf"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to define the query #4\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5laPFTHrFjVf"
      },
      "source": [
        "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQsR8SplFjVg"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to define the query #5\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5myZ5dOOefc"
      },
      "source": [
        "## 3 - Question Answering using LLM with Prompt Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMZqTudYBCWv"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"_____\" #Complete the code to define the system prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jg3r_LWOeff"
      },
      "source": [
        "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5zh3HQoOeff"
      },
      "outputs": [],
      "source": [
        "user_input = system_prompt+\"\\n\"+ \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "response(user_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYpyw4HjOeff"
      },
      "source": [
        "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPPpDM6cOeff"
      },
      "outputs": [],
      "source": [
        "user_input = _____ #Complete the code to pass the query #2\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRp92JQZOeff"
      },
      "source": [
        "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC6rrtblOefg"
      },
      "outputs": [],
      "source": [
        "user_input = _____ #Complete the code to pass the query #3\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA45zwyUOefg"
      },
      "source": [
        "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DrjIlKlOefg"
      },
      "outputs": [],
      "source": [
        "user_input = _____ #Complete the code to pass the query #4\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYXxiSuBOefg"
      },
      "source": [
        "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue8Lk8uXOefg"
      },
      "outputs": [],
      "source": [
        "user_input = _____ #Complete the code to pass the query #5\n",
        "response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_O1PGdNO2M9"
      },
      "source": [
        "## 4 - Data Preparation for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f4yFqLBINGG"
      },
      "outputs": [],
      "source": [
        "#Libraries for processing dataframes,text\n",
        "import json,os\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTpWESc53dL9"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5dLjPsyi1A4"
      },
      "outputs": [],
      "source": [
        "# uncomment and run the below code snippets if the dataset is present in the Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksv9hSCR4BM_"
      },
      "outputs": [],
      "source": [
        "manual_pdf_path = \"_____\" #Complete the code to define the file name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhf34I1eYNtR"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyMuPDFLoader(manual_pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YChLS31TxC3-"
      },
      "outputs": [],
      "source": [
        "manual = pdf_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffj0ca3eZT4u"
      },
      "source": [
        "### Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9weTDzMxRRS"
      },
      "source": [
        "#### Checking the first 5 pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSOv3q2pxX4z"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    print(f\"Page Number : {i+1}\",end=\"\\n\")\n",
        "    print(manual[i].page_content,end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-wNNalNxPKT"
      },
      "source": [
        "#### Checking the number of pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scMnmNKtxBea"
      },
      "outputs": [],
      "source": [
        "len(manual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LECMxTH-zB-R"
      },
      "source": [
        "### Data Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG0_pBmizGGt"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=_____, #Complete the code to define the chunk size\n",
        "    chunk_overlap= _____ #Complete the code to define the chunk overlap\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w76ji7ECzLLQ"
      },
      "outputs": [],
      "source": [
        "document_chunks = pdf_loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6TQ-mmLzR9I"
      },
      "outputs": [],
      "source": [
        "len(document_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuRoMIGNzZaJ"
      },
      "outputs": [],
      "source": [
        "document_chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xAQYLXqyCc6"
      },
      "outputs": [],
      "source": [
        "document_chunks[2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju4gtXwiyILp"
      },
      "outputs": [],
      "source": [
        "document_chunks[3].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl_9lAlGaYdf"
      },
      "source": [
        "As expected, there are some overlaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvHVejcWz0Bl"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6cZVZWQz15c"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformerEmbeddings(model_name=\"_____\") #Complete the code to define the model name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvc-CTtCz4kJ"
      },
      "outputs": [],
      "source": [
        "embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)\n",
        "embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0qy6xOZ0UBe"
      },
      "outputs": [],
      "source": [
        "print(\"Dimension of the embedding vector \",len(embedding_1))\n",
        "len(embedding_1)==len(embedding_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bmtFRTN0VtN"
      },
      "outputs": [],
      "source": [
        "embedding_1,embedding_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiKCOv4X0d7B"
      },
      "source": [
        "### Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWOqhGMV0kZ9"
      },
      "outputs": [],
      "source": [
        "out_dir = 'medical_db'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g-D3URW0iEO"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    _____, #Complete the code to pass the document chunks\n",
        "    _____, #Complete the code to pass the embedding model\n",
        "    persist_directory=out_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuK6hsbaGfqH"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZON_Uj1EeS"
      },
      "outputs": [],
      "source": [
        "vectorstore.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9HgsipF1I4H"
      },
      "outputs": [],
      "source": [
        "vectorstore.similarity_search(\"_____\",k=_____) #Complete the code to pass a query and an appropriate k value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEa5sKc41T1z"
      },
      "source": [
        "### Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO5kmp381VsX"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': _____} #Complete the code to pass an appropriate k value\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnKuqfWB1gBx"
      },
      "outputs": [],
      "source": [
        "rel_docs = retriever.get_relevant_documents(\"_____\") #Complete the code to pass the query\n",
        "rel_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjLbgBH83iQ6"
      },
      "outputs": [],
      "source": [
        "model_output = llm(\n",
        "      \"_____\", #Complete the code to pass the query\n",
        "      max_tokens=_____, #Complete the code to pass the maximum number of tokens\n",
        "      temperature=_____, #Complete the code to pass the temperature\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9K5_CD23zFY"
      },
      "outputs": [],
      "source": [
        "model_output['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YADTleB324vl"
      },
      "source": [
        "The above response is somewhat generic and is solely based on the data the model was trained on, rather than the medical manual.  \n",
        "\n",
        "Let's now provide our own context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw8qcwq66B0C",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### System and User Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wRkZYtO6B0D"
      },
      "source": [
        "Prompts guide the model to generate accurate responses. Here, we define two parts:\n",
        "\n",
        "    1. The system message describing the assistant's role.\n",
        "    2. A user message template including context and the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1737358838889
        },
        "id": "Dyl60SEs6B0D",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"_____\"  #Complete the code to define the system message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW38rWoNjJkQ"
      },
      "outputs": [],
      "source": [
        "qna_user_message_template = \"_____\" #Complete the code to define the user message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkIteX4m6mny"
      },
      "source": [
        "### Response Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-SfCZqC6B0E",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "\n",
        "    # Combine document chunks into a single context\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
        "    user_message = user_message.replace('{question}', user_input)\n",
        "\n",
        "    prompt = qna_system_message + '\\n' + user_message\n",
        "\n",
        "    # Generate the response\n",
        "    try:\n",
        "        response = llm(\n",
        "                  prompt=prompt,\n",
        "                  max_tokens=max_tokens,\n",
        "                  temperature=temperature,\n",
        "                  top_p=top_p,\n",
        "                  top_k=top_k\n",
        "                  )\n",
        "\n",
        "        # Extract and print the model's response\n",
        "        response = response['choices'][0]['text'].strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP1SRYbPQHN"
      },
      "source": [
        "## 5 - Question Answering using RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjajBEj06B0E"
      },
      "source": [
        "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt4TAQNa6B0E"
      },
      "outputs": [],
      "source": [
        "user_input = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "generate_rag_response(user_input,top_k=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDw8zXuq6B0F"
      },
      "source": [
        "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92cv0dQ6B0F"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #2\n",
        "generate_rag_response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TggYyQPL6B0G"
      },
      "source": [
        "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed6x6LGb6B0G"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #3\n",
        "generate_rag_response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TgxdI-_6B0G"
      },
      "source": [
        "### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7ru57_c6B0G"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #4\n",
        "generate_rag_response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlHXYCkm6B0H"
      },
      "source": [
        "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYIbiI_P6B0H"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #5\n",
        "generate_rag_response(_____) #Complete the code to pass the user input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7TYrqycEITB"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqPMPTQLUxel"
      },
      "source": [
        "#### Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIW5A_6rUxeo"
      },
      "outputs": [],
      "source": [
        "user_input = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "generate_rag_response(user_input,temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lP8LwnbUxep"
      },
      "source": [
        "#### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99zlqRbvUxeq"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #2\n",
        "generate_rag_response(_____) #Complete the code to pass the user input along with the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pludvG5BUxer"
      },
      "source": [
        "#### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CTt5MqmUxes"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #3\n",
        "generate_rag_response(_____) #Complete the code to pass the user input along with the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydSZJlJQUxes"
      },
      "source": [
        "#### Query 4:  What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pn3DJaUUxet"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #4\n",
        "generate_rag_response(_____) #Complete the code to pass the user input along with the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hAVjKHMUxeu"
      },
      "source": [
        "#### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD1T75NZUxev"
      },
      "outputs": [],
      "source": [
        "user_input_2 = \"_____\" #Complete the code to pass the query #5\n",
        "generate_rag_response(_____) #Complete the code to pass the user input along with the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyQrTipNfuBN"
      },
      "source": [
        "## 6 - Output Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbXMSxqa-65E"
      },
      "source": [
        "Let us now use the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation.\n",
        "\n",
        "- We are using the same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dDxkZdyKSnh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "groundedness_rater_system_message = \"_____\" #Complete the code to define the prompt to evaluate groundedness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIosu2Wk7OVs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "relevance_rater_system_message = \"_____\" #Complete the code to define the prompt to evaluate relevance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7boMupgh_Gux",
        "tags": []
      },
      "outputs": [],
      "source": [
        "user_message_template = \"\"\"\n",
        "###Question\n",
        "{question}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Answer\n",
        "{answer}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9UVEOZbXYlh"
      },
      "outputs": [],
      "source": [
        "def generate_ground_relevance_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
        "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    response = llm(\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            echo=False\n",
        "            )\n",
        "\n",
        "    answer =  response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    response_1 = llm(\n",
        "            prompt=groundedness_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            echo=False\n",
        "            )\n",
        "\n",
        "    response_2 = llm(\n",
        "            prompt=relevance_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            echo=False\n",
        "            )\n",
        "\n",
        "    return response_1['choices'][0]['text'],response_2['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6qxqyJLYA2x"
      },
      "source": [
        "### Query 1: What is the protocol for managing sepsis in a critical care unit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANzurSjuYA2x"
      },
      "outputs": [],
      "source": [
        "ground,rel = generate_ground_relevance_response(user_input=\"What is the protocol for managing sepsis in a critical care unit?\",max_tokens=370)\n",
        "\n",
        "print(ground,end=\"\\n\\n\")\n",
        "print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A60Q6x3YA2y"
      },
      "source": [
        "### Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOZQyoLwYA2y"
      },
      "outputs": [],
      "source": [
        "ground,rel = generate_ground_relevance_response(user_input=\"_____\",_____) #Complete the code to pass the query #2 along with parameters if needed\n",
        "\n",
        "print(ground,end=\"\\n\\n\")\n",
        "print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmYnriTdYA2z"
      },
      "source": [
        "### Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp898M2iYA2z"
      },
      "outputs": [],
      "source": [
        "ground,rel = generate_ground_relevance_response(user_input=\"_____\",_____) #Complete the code to pass the query #3 along with parameters if needed\n",
        "\n",
        "print(ground,end=\"\\n\\n\")\n",
        "print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz-lGsVxYA2z"
      },
      "source": [
        "### Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBRTYnFVYA2z"
      },
      "outputs": [],
      "source": [
        "ground,rel = generate_ground_relevance_response(user_input=\"_____\",_____) #Complete the code to pass the query #4 along with parameters if needed\n",
        "\n",
        "print(ground,end=\"\\n\\n\")\n",
        "print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2WxSxzDYA2z"
      },
      "source": [
        "### Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKCq09_YYA20"
      },
      "outputs": [],
      "source": [
        "ground,rel = generate_ground_relevance_response(user_input=\"_____\",_____) #Complete the code to pass the query #5 along with parameters if needed\n",
        "\n",
        "print(ground,end=\"\\n\\n\")\n",
        "print(rel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7QICRU-njdj"
      },
      "source": [
        "## Actionable Insights and Business Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObyXYhOojIaY"
      },
      "source": [
        "\n",
        "*   \n",
        "*  \n",
        "*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRlzaIhWaM9"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3CNz35ia6Bz3",
        "CkRbhMJH6Bz3",
        "CARPKFwm6Bz4",
        "by9EvAnkSpZf",
        "Uq1lhM4WFTS2",
        "EzzkvIXvFTS4",
        "K8YgK91SFjVY",
        "J6yxICeVFjVc",
        "oflaoOGiFjVd",
        "WUUqY4FbFjVe",
        "5laPFTHrFjVf",
        "9Jg3r_LWOeff",
        "iYpyw4HjOeff",
        "dRp92JQZOeff",
        "AA45zwyUOefg",
        "TYXxiSuBOefg",
        "t_O1PGdNO2M9",
        "ffj0ca3eZT4u",
        "f9weTDzMxRRS",
        "7-wNNalNxPKT",
        "LECMxTH-zB-R",
        "BvHVejcWz0Bl",
        "qiKCOv4X0d7B",
        "uEa5sKc41T1z",
        "vw8qcwq66B0C",
        "TkIteX4m6mny",
        "ffP1SRYbPQHN",
        "JjajBEj06B0E",
        "QDw8zXuq6B0F",
        "TggYyQPL6B0G",
        "1TgxdI-_6B0G",
        "FlHXYCkm6B0H",
        "K7TYrqycEITB",
        "rqPMPTQLUxel",
        "1lP8LwnbUxep",
        "pludvG5BUxer",
        "ydSZJlJQUxes",
        "5hAVjKHMUxeu",
        "f6qxqyJLYA2x",
        "7A60Q6x3YA2y",
        "ZmYnriTdYA2z",
        "jz-lGsVxYA2z",
        "x2WxSxzDYA2z",
        "Y7QICRU-njdj"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}